{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/facristiano/ic_sama/blob/main/data_management.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctlSmZDPs8Q9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, datetime, platform, logging, copy\n",
        "import numpy as np\n",
        "import pickle\n",
        "from itertools import product\n",
        "import pandas as pd\n",
        "\n",
        "def convert_file_path_os(path):\n",
        "    # this function simply converts the file path to correspond to the machine OS system\n",
        "    import platform\n",
        "    if system == 'Linux' or system == 'Darwin':  #1 confere se o sistema operacional é linux ou mac\n",
        "        path = path.replace('\\\\', '/')\n",
        "    return path\n",
        "\n",
        "def write_conf(folder, parameters, yml_file=None):\n",
        "    # this function saves the execution data into a .txt file on the simulation folder\n",
        "    with open(folder + 'exec_stats.txt', 'w') as f:\n",
        "        for key, dict in parameters.items():\n",
        "            f.writelines(str(key) + '\\n \\n')\n",
        "            for key2, item in parameters[key].items():\n",
        "                f.writelines(str(key2) + ': ' + str(item) + '\\n')\n",
        "            f.writelines('\\n')\n",
        "\n",
        "    if yml_file is not None:  # to save the yaml file (to rerun or resume an execution)\n",
        "        import shutil\n",
        "        shutil.copy(yml_file, folder)\n",
        "\n",
        "\n",
        "def macel_data_dict(data_dict_=None, data_=None, n_cells=None, n_samples=None, n_centers=None, dist_typ=None):\n",
        "    # this functions creates a dictionary for the simulations results be stored\n",
        "    # the raw data is not affected by this organization\n",
        "    if not data_ or not data_dict_:\n",
        "        # creating the base simplified data dict\n",
        "        data_dict_ = {'BSs': [], 'UEs': [], 'mean_snr': [], 'std_snr': [], 'mean_cap': [], 'std_cap': [], 'mean_user_time': [],\n",
        "                      'std_user_time': [], 'mean_user_bw': [], 'std_user_bw': [], 'raw_data': [], 'total_meet_criteria': [],\n",
        "                      'mean_deficit': [], 'std_deficit': [], 'mean_norm_deficit': [], 'std_norm_deficit': []}\n",
        "        data_dict_ = {'downlink_data': copy.deepcopy(data_dict_), 'uplink_data': copy.deepcopy(data_dict_)}  # replicating for uplink and downlink\n",
        "    else:\n",
        "\n",
        "        if data_[0]['downlink_results'] is not None:\n",
        "            downlink_data = [x['downlink_results'] for x in data_]\n",
        "            data_dict_['downlink_data'] = organize_data_matrix(data_=downlink_data,\n",
        "                                                               data_dict_=data_dict_['downlink_data'], n_cells=n_cells,\n",
        "                                                               n_samples=n_samples, n_centers=n_centers,\n",
        "                                                               dist_typ=dist_typ)\n",
        "\n",
        "\n",
        "        if data_[0]['uplink_results'] is not None:\n",
        "            uplink_data = [x['uplink_results'] for x in data_]\n",
        "            data_dict_['uplink_data'] = organize_data_matrix(data_=uplink_data,\n",
        "                                                             data_dict_=data_dict_['uplink_data'], n_cells=n_cells,\n",
        "                                                             n_samples=n_samples, n_centers=n_centers,\n",
        "                                                             dist_typ=dist_typ)\n",
        "\n",
        "\n",
        "    return data_dict_\n",
        "\n",
        "\n",
        "def organize_data_matrix(data_, data_dict_, n_cells, n_samples, n_centers, dist_typ):\n",
        "    # this funciton will take the simulation matrix data and will fit into the standard dictionary simulation output\n",
        "    snr_cap_stats = [x['snr_cap_stats'] for x in data_]\n",
        "    raw_data = [x['raw_data_dict'] for x in data_]\n",
        "    if dist_typ == 'uniform':\n",
        "        n_centers=1\n",
        "\n",
        "    # saving cumulative simple metrics\n",
        "    snr_cap_stats = np.array(snr_cap_stats)\n",
        "\n",
        "    data_dict_['BSs'].append(n_cells)\n",
        "    data_dict_['UEs'].append(n_samples * n_centers)\n",
        "    data_dict_['mean_snr'].append(np.mean([x['mean_snr'] for x in snr_cap_stats]))\n",
        "    data_dict_['std_snr'].append(np.mean([x['std_snr'] for x in snr_cap_stats]))\n",
        "    data_dict_['mean_cap'].append(np.mean([x['mean_cap'] for x in snr_cap_stats]))\n",
        "    data_dict_['std_cap'].append(np.mean([x['std_cap'] for x in snr_cap_stats]))\n",
        "    data_dict_['mean_user_time'].append(np.mean([x['mean_user_time'] for x in snr_cap_stats]))\n",
        "    data_dict_['std_user_time'].append(np.mean([x['std_user_time'] for x in snr_cap_stats]))\n",
        "    data_dict_['mean_user_bw'].append(np.mean([x['mean_user_bw'] for x in snr_cap_stats]))\n",
        "    data_dict_['std_user_bw'].append(np.mean([x['std_user_bw'] for x in snr_cap_stats]))\n",
        "\n",
        "    if 'total_meet_criteria' in snr_cap_stats[0]:  # in the case of not using the capacity criteria\n",
        "        data_dict_['total_meet_criteria'].append(np.mean([x['total_meet_criteria'] for x in snr_cap_stats]))\n",
        "        data_dict_['mean_deficit'].append(np.mean([x['mean_deficit'] for x in snr_cap_stats]))\n",
        "        data_dict_['std_deficit'].append(np.mean([x['std_deficit'] for x in snr_cap_stats]))\n",
        "        data_dict_['mean_norm_deficit'].append(np.mean([x['mean_norm_deficit'] for x in snr_cap_stats]))\n",
        "        data_dict_['std_norm_deficit'].append(np.mean([x['std_norm_deficit'] for x in snr_cap_stats]))\n",
        "\n",
        "    # storing the raw data\n",
        "    data_dict_['raw_data'].append(raw_data)\n",
        "\n",
        "    return data_dict_\n",
        "\n",
        "\n",
        "def create_subfolder(name_file, n_index, dict_name):\n",
        "    # this function creates a folder inside the simulation one (name_file) to store data\n",
        "    if not os.path.isdir('output'):\n",
        "        os.mkdir('output')\n",
        "    folder = os.path.dirname(__file__)\n",
        "    folder = folder.replace('/', '\\\\')\n",
        "    folder = '\\\\'.join(folder.split('\\\\')[:-1])\n",
        "    folder += '\\\\output\\\\' + name_file + '\\\\'\n",
        "\n",
        "    # creating subfolder\n",
        "    folder = folder + '\\\\' + str(n_index) + ' ' + dict_name + '\\\\'\n",
        "    convert_file_path_os(folder) #2 substituição da função if pela função convert_file_path_os(path)\n",
        "    if not os.path.exists(folder):\n",
        "        os.mkdir(folder)\n",
        "\n",
        "    return folder\n",
        "\n",
        "def load_data(name_file, return_path=False):\n",
        "    # this function will load a dictionary in a pickle file in the provided path folder\n",
        "    folder = os.path.dirname(__file__)\n",
        "    folder = folder.replace('/', '\\\\')\n",
        "    folder = '\\\\'.join(folder.split('\\\\')[:-1])\n",
        "    folder += '\\\\output\\\\' + name_file + '\\\\'\n",
        "\n",
        "    convert_file_path_os(folder) #3 substituição da função if pela função convert_file_path_os(path)\n",
        "\n",
        "    path = folder\n",
        "    folder += name_file + '.pkl'\n",
        "\n",
        "    try:\n",
        "        with open(folder, 'rb') as f:\n",
        "            data_dict = pickle.load(f)\n",
        "            f.close()\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "    if return_path:\n",
        "        return data_dict[0], path\n",
        "    else:\n",
        "        return data_dict[0]\n",
        "\n",
        "def save_data(path = None, data_dict = None):\n",
        "    # this function will save a dictionary in a pickle file in the provided path folder\n",
        "    if not path:\n",
        "        folder = os.path.dirname(__file__)\n",
        "        folder = folder.replace('/', '\\\\')\n",
        "        folder = '\\\\'.join(folder.split('\\\\')[:-1])\n",
        "        date = datetime.datetime.now()\n",
        "        name_file = date.strftime('%x') + '-' + date.strftime('%X')\n",
        "        name_file = name_file.replace('/', '_').replace(':', '_')\n",
        "        folder += '\\\\output\\\\' + name_file + '\\\\'\n",
        "        path = folder + name_file + '.pkl'\n",
        "\n",
        "        convert_file_path_os(folder) #4 substituição da função if pela função convert_file_path_os(path)\n",
        "        print(folder)\n",
        "        if os.path.exists(folder):\n",
        "            os.mkdir(folder)\n",
        "        else:\n",
        "            os.makedirs(folder)\n",
        "\n",
        "        return path, folder, name_file\n",
        "\n",
        "    else:\n",
        "        if data_dict and type(data_dict) is dict:\n",
        "            with open(path, 'wb') as f:\n",
        "                pickle.dump([data_dict], f)\n",
        "                f.close()\n",
        "                logging.info('Saved/updated file ' + path)\n",
        "        else:\n",
        "            logging.error('data_dictionary not provided!!!!')\n",
        "\n",
        "def extract_parameter_from_raw(raw_data, parameter_name, data_index, calc=None, concatenate=True):\n",
        "    # this function will pick the dictionary data and will organize and return the data for a specific parameter\n",
        "    if calc is None:\n",
        "        if concatenate:\n",
        "            extracted_data = np.concatenate([x[parameter_name] for x in raw_data[data_index]])\n",
        "        else:\n",
        "            extracted_data = [x[parameter_name] for x in raw_data[data_index]]\n",
        "    if calc == 'avg':\n",
        "        extracted_data = [x[parameter_name].mean() for x in raw_data[data_index]]\n",
        "    if calc == 'std':\n",
        "        extracted_data = [x[parameter_name].std() for x in raw_data[data_index]]\n",
        "    return extracted_data\n",
        "\n",
        "\n",
        "def group_ue(data_dict, iter_dict_name, data_index=None):\n",
        "    # this function will pick the output simulation data dict and will group the UEs by beam and sector and\n",
        "    # also indicates the UEs that was not connected to the network\n",
        "    dict = []\n",
        "    if data_index is None:\n",
        "        # bs_list = range(data_dict['BSs'].__len__())\n",
        "        iter_list = range(data_dict[iter_dict_name].__len__())\n",
        "    else:\n",
        "        iter_list = [data_index]\n",
        "\n",
        "    for bs_data_index in iter_list:\n",
        "        nactive_ue_cnt = []  # UEs non-connected to the network\n",
        "        ue_per_beam = []  # ues grouped by beam\n",
        "        ue_per_sector = []  # ues grouped by sector\n",
        "        active_ues = []  # UEs connected to the network\n",
        "\n",
        "        ue_bs_tables = [x['ue_bs_table'] for x in data_dict['raw_data'][bs_data_index]]\n",
        "        for i, ue_bs_tb in enumerate(ue_bs_tables):\n",
        "            beam_comb = np.array(list(product(ue_bs_tb['bs_index'].unique(), ue_bs_tb['beam_index'].unique(), ue_bs_tb['sector_index'].unique())))\n",
        "            sec_comb = np.array(list(product(ue_bs_tb['bs_index'].unique() , ue_bs_tb['sector_index'].unique())))\n",
        "            act_beams = beam_comb[(beam_comb[:, 0] != -1) & (beam_comb[:, 1] != - 1) & (beam_comb[:, 2] != -1)]\n",
        "            act_sec = sec_comb[(sec_comb[:, 0] != -1) & (sec_comb[:, 1] != - 1)]\n",
        "            nactive_ue_cnt.append(np.sum(ue_bs_tb['bs_index'] == -1))\n",
        "            active_ues.append(np.where(ue_bs_tb['bs_index'] != -1)[0])\n",
        "            ue_bs_tb = np.array(ue_bs_tb)\n",
        "            dummy_beam = []\n",
        "            dummy_sec = []\n",
        "            for index in act_beams:\n",
        "                dummy_beam.append(np.where((ue_bs_tb[:, 0] == index[0]) & (ue_bs_tb[:, 1] == index[1]) &\n",
        "                                            (ue_bs_tb[:, 2] == index[2]))[0])\n",
        "            ue_per_beam.append(dummy_beam)\n",
        "            for index in act_sec:\n",
        "                dummy_sec.append(np.where((ue_bs_tb[:, 0] == index[0]) & (ue_bs_tb[:, 2] == index[1]))[0])\n",
        "            ue_per_sector.append(dummy_sec)\n",
        "\n",
        "        dict.append({'nactive_ue_cnt': nactive_ue_cnt, 'active_ues': active_ues, 'ue_per_beam':ue_per_beam,\n",
        "                    'ue_per_sector': ue_per_sector})\n",
        "\n",
        "    return dict\n",
        "\n",
        "\n",
        "def ue_relative_index(data_dict, data_index=None):\n",
        "    # this function will convert a output reference and return the relative index inside the dictionary\n",
        "    if data_index is None:\n",
        "        iter_list, _ = range(data_dict['BSs'].__len__())\n",
        "    else:\n",
        "        iter_list = [data_index]\n",
        "\n",
        "    rel_index_tables = []\n",
        "\n",
        "    for data_index in iter_list:\n",
        "        ue_bs_tables = [x['ue_bs_table'] for x in data_dict['raw_data'][data_index]]\n",
        "        nbs_rel_index = []\n",
        "        for i, ue_bs_tb in enumerate(ue_bs_tables):\n",
        "            a = np.where(ue_bs_tb['bs_index'] != -1)[0]\n",
        "            b = np.where(a == a)[0]\n",
        "            nbs_rel_index.append(pd.DataFrame(data=np.array([a, b]).T, columns=['ue_bs_index', 'relative_index']))\n",
        "        rel_index_tables.append(nbs_rel_index)\n",
        "\n",
        "    return rel_index_tables\n",
        "\n",
        "def temp_data_save(zero_state=False, dict=None, batch_file=None):\n",
        "    # to delete the temporary .pkl files inside the temp folder\n",
        "    path = 'temp'\n",
        "    if not os.path.isdir(path):\n",
        "        os.mkdir(path)\n",
        "    if zero_state:\n",
        "        for f in os.listdir(path):\n",
        "            os.remove(os.path.join(path, f))\n",
        "    if dict is not None:\n",
        "        pass\n",
        "    if batch_file is not None:\n",
        "        index = batch_file['index']\n",
        "        data = batch_file['data']\n",
        "        file_path = path + '/batch' + str(index) + '.pkl'\n",
        "        with open(file_path, 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "            f.close()\n",
        "            logging.info('Saved/updated file ' + file_path)\n",
        "\n",
        "def temp_data_load():\n",
        "    # to load the temporary .pkl files inside the temp folder\n",
        "    path = 'temp'\n",
        "    data = []\n",
        "    if os.path.isdir(path):\n",
        "        for filename in os.listdir(path):\n",
        "            if filename.find('batch') != -1:\n",
        "                file_path = os.path.join(path, filename)\n",
        "                if os.path.isfile(file_path):\n",
        "                    with open(file_path, 'rb') as f:\n",
        "                        data_ = pickle.load(f)\n",
        "                    data = data + data_\n",
        "        return data\n",
        "\n",
        "    else:\n",
        "        print('temp folder not located!!!')\n",
        "\n",
        "def temp_data_delete(type):\n",
        "    # to delete the temporary .pkl files inside the temp folder\n",
        "    path = 'temp'\n",
        "    if not ((type == 'batch') or (type == 'dict')):\n",
        "        raise NameError('type not defined in temp_data_delete')\n",
        "\n",
        "    if os.path.isdir(path):\n",
        "        for filename in os.listdir(path):\n",
        "            if filename.find(type) != -1:\n",
        "                file_path = os.path.join(path, filename)\n",
        "                os.remove(file_path)\n"
      ],
      "metadata": {
        "id": "Fa2IiDjY0a4C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}